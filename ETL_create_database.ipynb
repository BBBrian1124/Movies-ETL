{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "import re\n",
    "\n",
    "from sqlalchemy import create_engine\n",
    "import psycopg2\n",
    "\n",
    "from config import db_password\n",
    "\n",
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1. Add the clean movie function that takes in the argument, \"movie\".\n",
    "\n",
    "def clean_movie(movie):\n",
    "    movie = dict(movie) #create a non-destructive copy\n",
    "    alt_titles = {}\n",
    "    # combine alternate titles into one column\n",
    "    for key in ['Also known as','Arabic','Cantonese','Chinese','French',\n",
    "                'Hangul','Hebrew','Hepburn','Japanese','Literally',\n",
    "                'Mandarin','McCune-Reischauer','Original title','Polish',\n",
    "                'Revised Romanization','Romanized','Russian',\n",
    "                'Simplified','Traditional','Yiddish']:\n",
    "        # If the key/column exists for the movie \n",
    "        if key in movie:\n",
    "            # add it to the alt_titles value \n",
    "            alt_titles[key] = movie[key]\n",
    "            # and remove the key/column from the movie\n",
    "            movie.pop(key)\n",
    "    # If the length of alt_title is > 0 (i.e has values) then it will become a column in our dataframe \n",
    "    if len(alt_titles) > 0:\n",
    "        movie['alt_titles'] = alt_titles\n",
    "\n",
    "    # We want to change the redundant column names into the one column so it is easier to read\n",
    "    def change_column_name(old_name, new_name):\n",
    "        if old_name in movie:\n",
    "            movie[new_name] = movie.pop(old_name)\n",
    "    change_column_name('Adaptation by', 'Writer(s)')\n",
    "    change_column_name('Country of origin', 'Country')\n",
    "    change_column_name('Directed by', 'Director')\n",
    "    change_column_name('Distributed by', 'Distributor')\n",
    "    change_column_name('Edited by', 'Editor(s)')\n",
    "    change_column_name('Length', 'Running time')\n",
    "    change_column_name('Original release', 'Release date')\n",
    "    change_column_name('Music by', 'Composer(s)')\n",
    "    change_column_name('Produced by', 'Producer(s)')\n",
    "    change_column_name('Producer', 'Producer(s)')\n",
    "    change_column_name('Productioncompanies ', 'Production company(s)')\n",
    "    change_column_name('Productioncompany ', 'Production company(s)')\n",
    "    change_column_name('Released', 'Release Date')\n",
    "    change_column_name('Release Date', 'Release date')\n",
    "    change_column_name('Screen story by', 'Writer(s)')\n",
    "    change_column_name('Screenplay by', 'Writer(s)')\n",
    "    change_column_name('Story by', 'Writer(s)')\n",
    "    change_column_name('Theme music composer', 'Composer(s)')\n",
    "    change_column_name('Written by', 'Writer(s)')\n",
    "\n",
    "    return movie"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2 Add the function that takes in three arguments;\n",
    "# Wikipedia data, Kaggle metadata, and MovieLens rating data (from Kaggle)\n",
    "\n",
    "def extract_transform_load():\n",
    "    # Read in the kaggle metadata and MovieLens ratings CSV files as Pandas DataFrames.\n",
    "    kaggle_metadata = pd.read_csv(kaggle_file, low_memory=False)\n",
    "    ratings = pd.read_csv(ratings_file)\n",
    "\n",
    "    # Open and read the Wikipedia data JSON file.\n",
    "    with open(wiki_file, mode='r') as file:\n",
    "        wiki_movies_raw_json = json.load(file)\n",
    "        \n",
    "    # 3. Write a list comprehension to filter out TV shows.\n",
    "    # Assumption is that movies have a director and no episodes \n",
    "    wiki_filtered_df = [movie for movie in wiki_movies_raw_json\n",
    "               if ('Director' in movie or 'Directed by' in movie)\n",
    "                   and 'imdb_link' in movie\n",
    "                   and 'No. of episodes' not in movie]\n",
    "    \n",
    "    # 4. Write a list comprehension to iterate through the cleaned wiki movies list\n",
    "    # and call the clean_movie function on each movie.\n",
    "    cleaned_movies = [clean_movie(cleaning_movies) for cleaning_movies in wiki_filtered_df]\n",
    "\n",
    "    # 5. Read in the cleaned movies list from Step 4 as a DataFrame.\n",
    "    wiki_movies_df = pd.DataFrame(cleaned_movies)\n",
    "\n",
    "    # 6. Write a try-except block to catch errors while extracting the IMDb ID using a regular expression string and\n",
    "    #  dropping any imdb_id duplicates. If there is an error, capture and print the exception.\n",
    "    try:\n",
    "        # The imdb links look like this https://www.imdb.com/title/tt1234567/,\" with \"tt1234567\" as the imdb_id\n",
    "        # Using the .str.extract function, we can code to extract tt followed by the 7 digits after \n",
    "        wiki_movies_df['imdb_id'] = wiki_movies_df['imdb_link'].str.extract(r'(tt\\d{7})')  \n",
    "        # Use the drop_duplicates function to remove duplicates in the 'imdb_id' subset\n",
    "        wiki_movies_df.drop_duplicates(subset='imdb_id', inplace=True)\n",
    "    except Exception as error:\n",
    "        print(error)\n",
    "\n",
    "    #  7. Write a list comprehension to keep the columns that don't have null values from the wiki_movies_df DataFrame.\n",
    "    # We can modify the list comprehension to where if the count of null values is < 90% of the total values we will keep those columns\n",
    "    # In otherwords, if the column has 90% null data we won't keep it \n",
    "    wiki_columns_to_keep = [column for column in wiki_movies_df.columns if wiki_movies_df[column].isnull().sum() < len(wiki_movies_df) * 0.9]\n",
    "    wiki_movies_df = wiki_movies_df[wiki_columns_to_keep]\n",
    "\n",
    "    # 8. Create a variable that will hold the non-null values from the “Box office” column.\n",
    "    box_office = wiki_movies_df['Box office'].dropna()\n",
    "    \n",
    "    # 9. Convert the box office data created in Step 8 to string values using the lambda and join functions.\n",
    "    box_office = box_office.apply(lambda x: ' '.join(x) if type(x) == list else x)\n",
    "\n",
    "    # 10. Write a regular expression to match the six elements of \"form_one\" of the box office data.\n",
    "    form_one = r'\\$\\s*\\d+\\.?\\d*\\s*[mb]illi?on'\n",
    "   \n",
    "    # 11. Write a regular expression to match the three elements of \"form_two\" of the box office data.\n",
    "    form_two = r'\\$\\s*\\d{1,3}(?:[,\\.]\\d{3})+(?!\\s[mb]illion)'\n",
    "    \n",
    "    # Remember, there are two main forms the box office data is written in: \n",
    "    # \"$123.4 million\" (or billion), and \"$123,456,789.\"\n",
    "    # Our above forms will allow us to match these \n",
    "\n",
    "    # 12. Add the parse_dollars function.\n",
    "    def parse_dollars(s):\n",
    "        # if s is not a string, return NaN\n",
    "        if type(s) != str:\n",
    "            return np.nan\n",
    "\n",
    "        # We'll use re.match(pattern, string) to see if our string matches a pattern\n",
    "        # we can slighthly modify form_one to check if input is of the form $###.# million\n",
    "        if re.match(r'\\$\\s*\\d+\\.?\\d*\\s*milli?on', s, flags=re.IGNORECASE):\n",
    "\n",
    "            # Next, we'll use re.sub(pattern, replacement_string, string) to remove dollar signs, spaces, commas, and letters\n",
    "                # this checks after the dollar sign for any non-white space characters or any characters and replaces it with blank\n",
    "            # remove dollar sign and \" million\"\n",
    "            s = re.sub('\\$|\\s|[a-zA-Z]','', s)\n",
    "\n",
    "            # convert to float and multiply by a million\n",
    "            value = float(s) * 10**6\n",
    "\n",
    "            # return value\n",
    "            return value\n",
    "\n",
    "        # The formula to covert the value to billion is different but we can use a similar code to above \n",
    "        # if input is of the form $###.# billion\n",
    "        elif re.match(r'\\$\\s*\\d+\\.?\\d*\\s*billi?on', s, flags=re.IGNORECASE):\n",
    "\n",
    "            # remove dollar sign and \" billion\"\n",
    "            s = re.sub('\\$|\\s|[a-zA-Z]','', s)\n",
    "\n",
    "            # convert to float and multiply by a billion\n",
    "            value = float(s) * 10**9\n",
    "\n",
    "            # return value\n",
    "            return value\n",
    "\n",
    "        # if input is of the form $###,###,###\n",
    "        elif re.match(r'\\$\\s*\\d{1,3}(?:[,\\.]\\d{3})+(?!\\s[mb]illion)', s, flags=re.IGNORECASE):\n",
    "\n",
    "            # remove dollar sign and commas\n",
    "            s = re.sub('\\$|,','', s)\n",
    "\n",
    "            # convert to float\n",
    "            value = float(s)\n",
    "\n",
    "            # return value\n",
    "            return value\n",
    "\n",
    "        # otherwise, return NaN\n",
    "        else:\n",
    "            return np.nan\n",
    "\n",
    "        \n",
    "    # 13. Clean the box office column in the wiki_movies_df DataFrame.\n",
    "    #box_office = box_office.str.replace(r'\\$.*[-—–](?![a-z])', '$', regex=True)\n",
    "    wiki_movies_df['box_office'] = box_office.str.extract(f'({form_one}|{form_two})', flags=re.IGNORECASE)[0].apply(parse_dollars)\n",
    "    \n",
    "    # We no longer need the Box Office column, so we'll just drop it\n",
    "    wiki_movies_df.drop('Box office', axis=1, inplace=True)\n",
    "    \n",
    "    # 14. Clean the budget column in the wiki_movies_df DataFrame.\n",
    "    # First create a variable to store budget once we drop the NA values \n",
    "    budget = wiki_movies_df['Budget'].dropna()\n",
    "    \n",
    "    # Convert any lists to strings using the .join method \n",
    "    budget = budget.map(lambda x: ' '.join(x) if type(x) == list else x)\n",
    "    \n",
    "    # Remove values between the dollar sign and hypen (for budgets displayed in range) using the same code as for box office \n",
    "    budget = budget.str.replace(r'\\$.*[-—–](?![a-z])', '$', regex=True)\n",
    "    \n",
    "    # We notice that there are values that show a # within a bracket such as $40 [4] million which are just citation references \n",
    "    # We can use this code to replace it with a blank\n",
    "    budget = budget.str.replace(r'\\[\\d+\\]\\s*', '')\n",
    "    \n",
    "    # Extract values that match our form and place it in the dataframe \n",
    "    wiki_movies_df['budget'] = budget.str.extract(f'({form_one}|{form_two})', flags=re.IGNORECASE)[0].apply(parse_dollars)\n",
    "    \n",
    "    # 15. Clean the release date column in the wiki_movies_df DataFrame.\n",
    "    # Drop NA values and convert lists to string \n",
    "    release_date = wiki_movies_df['Release date'].dropna().apply(lambda x: ' '.join(x) if type(x) == list else x)\n",
    "    \n",
    "    # The forms we'll be parsing are:\n",
    "    # A. Full month name, one- to two-digit day, four-digit year (i.e., January 1, 2000)\n",
    "    date_form_one = r'(?:January|February|March|April|May|June|July|August|September|October|November|December)\\s\\d{1,2},\\s\\d{4}'\n",
    "    # B. Four-digit year, two-digit month, two-digit day, with any separator (i.e., 2000-01-01)\n",
    "    date_form_two = r'\\d{4}.[01]\\d.[0123]\\d'\n",
    "    # C. Full month name, four-digit year (i.e., January 2000)\n",
    "    date_form_three = r'(?:January|February|March|April|May|June|July|August|September|October|November|December)\\s\\d{4}'\n",
    "    # D. Four-digit year\n",
    "    date_form_four = r'\\d{4}'\n",
    "    \n",
    "    # Once we have the 4 forms, we can extract data that fit under those 4 forms \n",
    "    release_date.str.extract(f'({date_form_one}|{date_form_two}|{date_form_three}|{date_form_four})', flags=re.IGNORECASE)\n",
    "    \n",
    "    # We can then use the to_datetime method to convert our data into dates\n",
    "    # set the infer_datetime_format option to True so that it infers the date when it isn't fully specified (i.e. form 4)\n",
    "    wiki_movies_df['release_date'] = pd.to_datetime(release_date.str.extract(f'({date_form_one}|{date_form_two}|{date_form_three}|{date_form_four})')[0], infer_datetime_format=True)\n",
    "                                                    \n",
    "    # 16. Clean the running time column in the wiki_movies_df DataFrame.\n",
    "    # First drop any NA values and place any lists into a string \n",
    "    running_time = wiki_movies_df['Running time'].dropna().apply(lambda x: ' '.join(x) if type(x) == list else x)\n",
    "   \n",
    "    # We will extract the data that meets our form\n",
    "    running_time_extract = running_time.str.extract(r'(\\d+)\\s*ho?u?r?s?\\s*(\\d*)|(\\d+)\\s*m')\n",
    "    \n",
    "    # Using to_numeric we can convert strings to numeric values \n",
    "    # Coercing the errors will turn the empty strings into Not a Number (NaN)\n",
    "    # then we can use fillna() to change all the NaNs to zeros.\n",
    "    running_time_extract = running_time_extract.apply(lambda col: pd.to_numeric(col, errors='coerce')).fillna(0)\n",
    "    \n",
    "    # Now we can apply a function that will convert the hour capture groups and minute capture groups to minutes \n",
    "    # if the pure minutes capture group is zero, and save the output to wiki_movies_df\n",
    "    wiki_movies_df['running_time'] = running_time_extract.apply(lambda row: row[0]*60 + row[1] if row[2] == 0 else row[2], axis=1)                                                \n",
    "    \n",
    "    # Drop the running time column because we don't need it \n",
    "    wiki_movies_df.drop('Running time', axis=1, inplace=True)\n",
    "\n",
    "    \n",
    "# DELIVERABLE 3 \n",
    "\n",
    "    # 2. Clean the Kaggle metadata.\n",
    "    \n",
    "    # Reviewing again, we probably don't want adult movies in our dataset so we keep values where adult is false, and drop the rest\n",
    "    kaggle_metadata = kaggle_metadata[kaggle_metadata['adult'] == 'False'].drop('adult',axis='columns')\n",
    "    \n",
    "    # This converts the column to a boolean\n",
    "    kaggle_metadata['video'] = kaggle_metadata['video'] == True\n",
    "\n",
    "    # This converts the other columns into numeric \n",
    "    # using errors = 'raise' will let us know if any data could not be converted \n",
    "    kaggle_metadata['budget'] = kaggle_metadata['budget'].astype(int)\n",
    "    kaggle_metadata['id'] = pd.to_numeric(kaggle_metadata['id'], errors='raise')\n",
    "    kaggle_metadata['popularity'] = pd.to_numeric(kaggle_metadata['popularity'], errors='raise')\n",
    "    # Using Panda's built in .to_datetime function will convert the data into date\n",
    "    kaggle_metadata['release_date'] = pd.to_datetime(kaggle_metadata['release_date'])\n",
    "\n",
    "\n",
    "    # 3. Merged the two DataFrames into the movies DataFrame.\n",
    "    # Using the suffixes will allow us to show which dataframe the data comes from\n",
    "    movies_df = pd.merge(wiki_movies_df, kaggle_metadata, on='imdb_id', suffixes=['_wiki','_kaggle'])\n",
    "\n",
    "    # 4. Drop unnecessary columns from the merged DataFrame.\n",
    "    # First we will drop the wiki columns we have decided to drop \n",
    "    movies_df.drop(columns=['title_wiki','release_date_wiki','Language','Production company(s)'], inplace=True)\n",
    "\n",
    "    # 5. Add in the function to fill in the missing Kaggle data.\n",
    "    # In order to keep kaggle and fill 0s with wiki data we will create a fuction\n",
    "    # This will take a df and 2 \"columns\" as paramaters\n",
    "    # It will apply to the kaggle_column where if the value is 0 it will take the wiki_column else keep the kaggle_column\n",
    "    # It will then drop the wiki_column\n",
    "    def fill_missing_kaggle_data(df, kaggle_column, wiki_column):\n",
    "        df[kaggle_column] = df.apply(\n",
    "            lambda row: row[wiki_column] if row[kaggle_column] == 0 else row[kaggle_column]\n",
    "            , axis=1)\n",
    "        df.drop(columns=wiki_column, inplace=True)\n",
    "\n",
    "    # 6. Call the function in Step 5 with the DataFrame and columns as the arguments.\n",
    "    # We can then call our function to keep kaggle data and fill in 0s with wikipedia data \n",
    "    fill_missing_kaggle_data(movies_df, 'runtime', 'running_time')\n",
    "    fill_missing_kaggle_data(movies_df, 'budget_kaggle', 'budget_wiki')\n",
    "    fill_missing_kaggle_data(movies_df, 'revenue', 'box_office')\n",
    "\n",
    "    # 7. Filter the movies DataFrame for specific columns.\n",
    "    # Use the .loc method to re-arrange the columns\n",
    "    movies_df = movies_df.loc[:, ['imdb_id','id','title_kaggle','original_title','tagline','belongs_to_collection','url','imdb_link',\n",
    "                           'runtime','budget_kaggle','revenue','release_date_kaggle','popularity','vote_average','vote_count',\n",
    "                           'genres','original_language','overview','spoken_languages','Country',\n",
    "                           'production_companies','production_countries','Distributor',\n",
    "                           'Producer(s)','Director','Starring','Cinematography','Editor(s)','Writer(s)','Composer(s)','Based on'\n",
    "                          ]]\n",
    "\n",
    "    # We want to check if there are any columns that hold one value since it won't provide us much information \n",
    "    # the result of this code tells us that the video column only holds one value \"false\" where as the other columns have various info\n",
    "    [col for col in movies_df.columns if len(movies_df[col].apply(lambda x: tuple(x) if type(x) == list else x).value_counts(dropna=False)) == 1]\n",
    "    \n",
    "    # 8. Rename the columns in the movies DataFrame.\n",
    "    # Then use the .rename function to rename the columns to be more consistent \n",
    "    movies_df.rename({'id':'kaggle_id',\n",
    "                      'title_kaggle':'title',\n",
    "                      'url':'wikipedia_url',\n",
    "                      'budget_kaggle':'budget',\n",
    "                      'release_date_kaggle':'release_date',\n",
    "                      'Country':'country',\n",
    "                      'Distributor':'distributor',\n",
    "                      'Producer(s)':'producers',\n",
    "                      'Director':'director',\n",
    "                      'Starring':'starring',\n",
    "                      'Cinematography':'cinematography',\n",
    "                      'Editor(s)':'editors',\n",
    "                      'Writer(s)':'writers',\n",
    "                      'Composer(s)':'composers',\n",
    "                      'Based on':'based_on'\n",
    "                     }, axis='columns', inplace=True)\n",
    "\n",
    "\n",
    "    # 9. Transform and merge the ratings DataFrame.\n",
    "    # Convert Unix dates to regular date format\n",
    "    ratings['timestamp'] = pd.to_datetime(ratings['timestamp'], unit='s')\n",
    "    # Finally we can use the pivot function to make the index the movieID then the columns as rating and the value as count \n",
    "    rating_counts = ratings.groupby(['movieId','rating'], as_index=False).count() \\\n",
    "                    .rename({'userId':'count'}, axis=1) \\\n",
    "                    .pivot(index='movieId',columns='rating', values='count')\n",
    "    \n",
    "    # We can then rename the column headers to show rating_ followed by the rating scale value \n",
    "    rating_counts.columns = ['rating_' + str(col) for col in rating_counts.columns]\n",
    "    \n",
    "    # We can then use a left merge to keep everything in the movies_df but merge in these rating counts\n",
    "    movies_with_ratings_df = pd.merge(movies_df, rating_counts, left_on='kaggle_id', right_index=True, how='left')\n",
    "\n",
    "    # Since there may be some movies that don't have ratings on that scale/#, we'd want to fill those in with 0 \n",
    "    movies_with_ratings_df[rating_counts.columns] = movies_with_ratings_df[rating_counts.columns].fillna(0)\n",
    "\n",
    "    #return wiki_movies_df, movies_with_ratings_df, movies_df\n",
    "\n",
    "# DELIVERABLE 4: \n",
    "    # We can create the link to our sql database using this syntax\n",
    "    # Remember, we shouldn't type passwords as code so we will store it in our config.py file which will not be uploaded into github\n",
    "    \"postgres://[user]:[password]@[location]:[port]/[database]\"\n",
    "\n",
    "    # We will use that syntax and call our password to import into our port and database\n",
    "    db_string = f\"postgres://postgres:{db_password}@127.0.0.1:5432/Challenge: movie_data\"\n",
    "\n",
    "    # This is all the information that SQLAlchemy needs to create a database engine.\n",
    "    # SQLAlchemy handles connections to different SQL databases and manages the conversion between data types. \n",
    "    # The way it handles all the communication and conversion is by creating a database engine\n",
    "    engine = create_engine(db_string)\n",
    "\n",
    "    # Some users may need an additional package installed for this section. \n",
    "    # In your terminal, run the following code: pip install psycopg2-binary to add it to your coding environment.\n",
    "\n",
    "    # Import the movie df to sql\n",
    "    # we need to specify the name of the table and the engine in the to_sql() method.\n",
    "    movies_df.to_sql(name='movies', con=engine)\n",
    "\n",
    "    # create a variable for the number of rows imported\n",
    "    rows_imported = 0\n",
    "    # get the start_time from time.time()\n",
    "    # .time() stores the time when it is called \n",
    "    start_time = time.time()\n",
    "\n",
    "    for data in pd.read_csv(f'{file_dir}/ratings.csv', chunksize=1000000):\n",
    "\n",
    "        # print out the range of rows that are being imported\n",
    "        # This tells us when import has started and ended \n",
    "        # use the end= parameter in the print function. \n",
    "            # Setting the end to an empty string will prevent the output from going to the next line.\n",
    "        print(f'importing rows {rows_imported} to {rows_imported + len(data)}...', end='')\n",
    "\n",
    "        data.to_sql(name='ratings', con=engine, if_exists='replace')\n",
    "\n",
    "        # increment the number of rows imported by the size of 'data'\n",
    "        rows_imported += len(data)\n",
    "\n",
    "        # add elapsed time to final print out\n",
    "        # print that the rows have finished importing\n",
    "        print(f'Done. {time.time() - start_time} total seconds elapsed')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 10. Create the path to your file directory and variables for the three files.\n",
    "file_dir = 'C://Users/100556036/Downloads/UofT_Class_Folder/Module_8_ETL/Movies-ETL/Challenge/Resources'\n",
    "# The Wikipedia data\n",
    "wiki_file = f'{file_dir}/wikipedia-movies.json'\n",
    "# The Kaggle metadata\n",
    "kaggle_file = f'{file_dir}/movies_metadata.csv'\n",
    "# The MovieLens rating data.\n",
    "ratings_file = f'{file_dir}/ratings.csv'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "importing rows 0 to 1000000...Done. 154.41036343574524 total seconds elapsed\n",
      "importing rows 1000000 to 2000000...Done. 256.36280727386475 total seconds elapsed\n",
      "importing rows 2000000 to 3000000...Done. 348.9034192562103 total seconds elapsed\n",
      "importing rows 3000000 to 4000000...Done. 441.7857971191406 total seconds elapsed\n",
      "importing rows 4000000 to 5000000...Done. 534.1046726703644 total seconds elapsed\n",
      "importing rows 5000000 to 6000000...Done. 627.4117023944855 total seconds elapsed\n",
      "importing rows 6000000 to 7000000...Done. 720.0915493965149 total seconds elapsed\n",
      "importing rows 7000000 to 8000000...Done. 813.2584228515625 total seconds elapsed\n",
      "importing rows 8000000 to 9000000...Done. 906.0936703681946 total seconds elapsed\n",
      "importing rows 9000000 to 10000000...Done. 999.7310209274292 total seconds elapsed\n",
      "importing rows 10000000 to 11000000...Done. 1092.8619666099548 total seconds elapsed\n",
      "importing rows 11000000 to 12000000...Done. 1186.1735708713531 total seconds elapsed\n",
      "importing rows 12000000 to 13000000...Done. 1279.2905979156494 total seconds elapsed\n",
      "importing rows 13000000 to 14000000...Done. 1372.3639409542084 total seconds elapsed\n",
      "importing rows 14000000 to 15000000...Done. 1467.9752233028412 total seconds elapsed\n",
      "importing rows 15000000 to 16000000...Done. 1574.4572842121124 total seconds elapsed\n",
      "importing rows 16000000 to 17000000...Done. 1674.5623695850372 total seconds elapsed\n",
      "importing rows 17000000 to 18000000...Done. 1779.3921954631805 total seconds elapsed\n",
      "importing rows 18000000 to 19000000...Done. 1887.2220251560211 total seconds elapsed\n",
      "importing rows 19000000 to 20000000...Done. 1987.8452196121216 total seconds elapsed\n",
      "importing rows 20000000 to 21000000...Done. 2097.3786165714264 total seconds elapsed\n",
      "importing rows 21000000 to 22000000...Done. 2203.1827120780945 total seconds elapsed\n",
      "importing rows 22000000 to 23000000...Done. 2310.6242456436157 total seconds elapsed\n",
      "importing rows 23000000 to 24000000...Done. 2412.1221795082092 total seconds elapsed\n",
      "importing rows 24000000 to 25000000...Done. 2509.253697872162 total seconds elapsed\n",
      "importing rows 25000000 to 26000000...Done. 2607.6477024555206 total seconds elapsed\n",
      "importing rows 26000000 to 26024289...Done. 2610.0354471206665 total seconds elapsed\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "cannot unpack non-iterable NoneType object",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-5-f8543da1323a>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[1;31m# 11. Set the three variables equal to the function created in D1.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 2\u001b[1;33m \u001b[0mwiki_file\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mkaggle_file\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mratings_file\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mextract_transform_load\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m: cannot unpack non-iterable NoneType object"
     ]
    }
   ],
   "source": [
    "# 11. Set the three variables equal to the function created in D1.\n",
    "wiki_file, kaggle_file, ratings_file = extract_transform_load()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "PythonData",
   "language": "python",
   "name": "pythondata"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
